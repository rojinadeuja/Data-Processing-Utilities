{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSV-to-Corpus.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP2LxgBH6vp/DWy6ux5g13u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rojinadeuja/Data-Processing-Utilities/blob/main/CSV_to_Corpus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFUqmF8TRisU",
        "outputId": "e3e606fb-1d63-4f0d-a68a-904107c81d8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZPSNPacUQ6_"
      },
      "source": [
        "## Corpus\n",
        "A corpus is a collection of text documents, and corpora is the plural of corpus. So a custom corpus is really just a bunch of text files in a directory, often alongside many other directories of text files.\n",
        "\n",
        "To train your own vectors, first you'll need to prepare your corpus as a single text file with all words separated by one or more spaces or tabs. If your corpus has multiple documents, the documents (only) should be separated by new line characters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b39dq6c6UX04"
      },
      "source": [
        "## Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "falQrAwfTMOT"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tl_NJ4KqXxiZ"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NF5cMb6rUXOl",
        "outputId": "6d54555b-ff5a-4720-a8d3-824d42aeec2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/IMDB.csv')\n",
        "df.head()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qgiC6wFX1T7"
      },
      "source": [
        "## Create Document array using text field"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZdBXh7_VVj7",
        "outputId": "5b26217e-003a-4a93-cf4d-58bb5350a93f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "docs_array = df['review']\n",
        "print(\"Dimension of the documents array: \", docs_array.shape)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dimension of the documents array:  (50000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raM9iNOwZ3bY"
      },
      "source": [
        "# Function for convert a list of sentences to a list of lists containing tokenized words\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "def docs_preprocessor(docs):\n",
        "    tokenizer = RegexpTokenizer(r'\\w+') # Tokenize the words.\n",
        "    \n",
        "    for idx in range(len(docs)):\n",
        "        docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
        "        docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
        "\n",
        "    # Remove numbers, but not words that contain numbers.\n",
        "    docs = [[token for token in doc if not token.isdigit()] for doc in docs]\n",
        "    \n",
        "    # Remove words that are only one character.\n",
        "    docs = [[token for token in doc if len(token) > 3] for doc in docs]\n",
        "    \n",
        "    # Lemmatize all words in documents.\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]\n",
        "  \n",
        "    return docs"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCU4VuQEaf2e",
        "outputId": "89aa3157-aa17-4cc0-8a93-0d9d614a7f00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "# Convert a list of sentences to a list of lists containing tokenized words\n",
        "%time docs = docs_preprocessor(docs_array)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "CPU times: user 1min 28s, sys: 1.01 s, total: 1min 29s\n",
            "Wall time: 1min 29s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXAPcWkIah2N",
        "outputId": "8ce2ad58-1de8-442d-e3d4-96417e21f917",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"Length of the 2D Array of Tokenized Documents: \", len(docs))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of the 2D Array of Tokenized Documents:  50000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5tiyUha95Vx",
        "outputId": "11bbfa44-3e11-4ec8-d20a-75fb4329f46a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "docs[1]"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['wonderful',\n",
              " 'little',\n",
              " 'production',\n",
              " 'filming',\n",
              " 'technique',\n",
              " 'very',\n",
              " 'unassuming',\n",
              " 'very',\n",
              " 'time',\n",
              " 'fashion',\n",
              " 'give',\n",
              " 'comforting',\n",
              " 'sometimes',\n",
              " 'discomforting',\n",
              " 'sense',\n",
              " 'realism',\n",
              " 'entire',\n",
              " 'piece',\n",
              " 'actor',\n",
              " 'extremely',\n",
              " 'well',\n",
              " 'chosen',\n",
              " 'michael',\n",
              " 'sheen',\n",
              " 'only',\n",
              " 'polari',\n",
              " 'voice',\n",
              " 'down',\n",
              " 'truly',\n",
              " 'seamless',\n",
              " 'editing',\n",
              " 'guided',\n",
              " 'reference',\n",
              " 'williams',\n",
              " 'diary',\n",
              " 'entry',\n",
              " 'only',\n",
              " 'well',\n",
              " 'worth',\n",
              " 'watching',\n",
              " 'terrificly',\n",
              " 'written',\n",
              " 'performed',\n",
              " 'piece',\n",
              " 'masterful',\n",
              " 'production',\n",
              " 'about',\n",
              " 'great',\n",
              " 'master',\n",
              " 'comedy',\n",
              " 'life',\n",
              " 'realism',\n",
              " 'really',\n",
              " 'come',\n",
              " 'home',\n",
              " 'with',\n",
              " 'little',\n",
              " 'thing',\n",
              " 'fantasy',\n",
              " 'guard',\n",
              " 'which',\n",
              " 'rather',\n",
              " 'than',\n",
              " 'traditional',\n",
              " 'dream',\n",
              " 'technique',\n",
              " 'remains',\n",
              " 'solid',\n",
              " 'then',\n",
              " 'disappears',\n",
              " 'play',\n",
              " 'knowledge',\n",
              " 'sens',\n",
              " 'particularly',\n",
              " 'with',\n",
              " 'scene',\n",
              " 'concerning',\n",
              " 'orton',\n",
              " 'halliwell',\n",
              " 'set',\n",
              " 'particularly',\n",
              " 'their',\n",
              " 'flat',\n",
              " 'with',\n",
              " 'halliwell',\n",
              " 'mural',\n",
              " 'decorating',\n",
              " 'every',\n",
              " 'surface',\n",
              " 'terribly',\n",
              " 'well',\n",
              " 'done']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzkNucZ-alsM"
      },
      "source": [
        "from itertools import chain\n",
        "# docs_= list(chain.from_iterable(docs))\n",
        "# len(docs_)"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fnQJzSkeCaU"
      },
      "source": [
        "# def convert(lst): \n",
        "#     return ' '.join(lst)"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8IxtK98e9D1",
        "outputId": "99cc1bec-6dbb-4169-c6a0-17558d3dcea6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        }
      },
      "source": [
        "# for i in docs:\n",
        "#   convert(i)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-bce9558f8231>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdocs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-64-7d201cbf2b5c>\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(lst)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: sequence item 0: expected str instance, list found"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QryPTBofBDj",
        "outputId": "f535af69-6dfd-4eb6-e6da-c761e9241e4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "docs_[1:1000]"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ther reviewer mentioned that after watching just episode hooked they right this exactly what happened with first thing that struck about brutality unflinching scene violence which right from word trust this show faint hearted timid this show pull punch with regard drug violence hardcore classic word called that nickname given oswald maximum security state penitentary focus mainly emerald city experimental section prison where cell have glass front face inwards privacy high agenda city home many aryan muslim gangsta latino christian italian irish more scuffle death stare dodgy dealing shady agreement never away would main appeal show fact that go where other show wouldn dare forget pretty picture painted mainstream audience forget charm forget romance doesn mess around first episode ever struck nasty surreal couldn ready watched more developed taste accustomed high level graphic violence just violence injustice crooked guard sold nickel inmate kill order away with well mannered middle '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbfZkFcHfi3L"
      },
      "source": [
        "# Open a file in witre mode\n",
        "fo = open(\"foo.txt\", \"w\")"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTZPcb0kgtHW"
      },
      "source": [
        "fo.writelines(docs_)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAIs0Lueg4Cb"
      },
      "source": [
        "with open('corpus.txt', 'w') as f:\n",
        "  for item in docs:\n",
        "      item_ = ' '.join(item)\n",
        "      f.write(\"%s\\n\" % item_)"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0OvKSl6JAa0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}